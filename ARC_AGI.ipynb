{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.14",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 67357,
          "databundleVersionId": 8951125,
          "sourceType": "competition"
        }
      ],
      "dockerImageVersionId": 30761,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "name": "ARC AGI",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shravya1998/MLProjects/blob/main/ARC_AGI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "\n",
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES\n",
        "# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from tempfile import NamedTemporaryFile\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import unquote, urlparse\n",
        "from urllib.error import HTTPError\n",
        "from zipfile import ZipFile\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "CHUNK_SIZE = 40960\n",
        "DATA_SOURCE_MAPPING = 'arc-prize-2024:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-competitions-data%2Fkaggle-v2%2F67357%2F8951125%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240911%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240911T213622Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D11cdeab9216626a7e4be6c39e6c5587c4c385e845431b97e23a32bb25708dd3d699646bd2a262121e28942c38a38918536cfc58684911ac9e28a205870d52ec73ada90b444dcb60a4e35be7c78713acebd3a328621557daa98b8428e14213d7c8c4ccd160e24458110c0cdac606090e86afc24ed518c871cb7e820c709792c6ed98c22718bb36781cdb1e8742d304c284f1e00aa5d1f63d7d1c133e92d9d9eecd07d715883609b245a0f2f1ea539cca2e800fc980eb8a179b348a88c61da545d0e8deaec572f2e9f9a2c03e1704e4b736f17cf3d5b1650523727834415d328a76462940244bc8571b5c48c7aebb8039779ceec5ba2b11f0dca6a5265b86825c1'\n",
        "\n",
        "KAGGLE_INPUT_PATH='/kaggle/input'\n",
        "KAGGLE_WORKING_PATH='/kaggle/working'\n",
        "KAGGLE_SYMLINK='kaggle'\n",
        "\n",
        "!umount /kaggle/input/ 2> /dev/null\n",
        "shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
        "os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
        "os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
        "\n",
        "try:\n",
        "  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "try:\n",
        "  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
        "    directory, download_url_encoded = data_source_mapping.split(':')\n",
        "    download_url = unquote(download_url_encoded)\n",
        "    filename = urlparse(download_url).path\n",
        "    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
        "    try:\n",
        "        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
        "            total_length = fileres.headers['content-length']\n",
        "            print(f'Downloading {directory}, {total_length} bytes compressed')\n",
        "            dl = 0\n",
        "            data = fileres.read(CHUNK_SIZE)\n",
        "            while len(data) > 0:\n",
        "                dl += len(data)\n",
        "                tfile.write(data)\n",
        "                done = int(50 * dl / int(total_length))\n",
        "                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
        "                sys.stdout.flush()\n",
        "                data = fileres.read(CHUNK_SIZE)\n",
        "            if filename.endswith('.zip'):\n",
        "              with ZipFile(tfile) as zfile:\n",
        "                zfile.extractall(destination_path)\n",
        "            else:\n",
        "              with tarfile.open(tfile.name) as tarfile:\n",
        "                tarfile.extractall(destination_path)\n",
        "            print(f'\\nDownloaded and uncompressed: {directory}')\n",
        "    except HTTPError as e:\n",
        "        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
        "        continue\n",
        "    except OSError as e:\n",
        "        print(f'Failed to load {download_url} to path {destination_path}')\n",
        "        continue\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "_I-DZ9igBB_7"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "execution": {
          "iopub.status.busy": "2024-09-10T20:14:12.349842Z",
          "iopub.execute_input": "2024-09-10T20:14:12.350281Z",
          "iopub.status.idle": "2024-09-10T20:14:12.818754Z",
          "shell.execute_reply.started": "2024-09-10T20:14:12.350237Z",
          "shell.execute_reply": "2024-09-10T20:14:12.817402Z"
        },
        "trusted": true,
        "id": "HSeM4rIhBB_9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load ARC dataset\n",
        "def load_arc_data(file_path):\n",
        "    with open(file_path, 'r') as f:\n",
        "        return json.load(f)\n",
        "\n",
        "# Function to extract train and test data from a task\n",
        "def extract_task_data(task):\n",
        "    train_inputs = [np.array(pair[\"input\"]) for pair in task[\"train\"]]\n",
        "    train_outputs = [np.array(pair[\"output\"]) for pair in task[\"train\"]]\n",
        "    test_inputs = [np.array(test[\"input\"]) for test in task[\"test\"]]\n",
        "    return train_inputs, train_outputs, test_inputs\n",
        "\n",
        "# Pad grids to the maximum size of 30x30\n",
        "def pad_grid(grid, target_size=(30, 30)):\n",
        "    padded_grid = np.zeros(target_size, dtype=int)\n",
        "    grid_height, grid_width = grid.shape\n",
        "    padded_grid[:grid_height, :grid_width] = grid\n",
        "    return padded_grid\n",
        "\n",
        "# Prepare the data for model input\n",
        "def prepare_data(tasks):\n",
        "    x_train, y_train = [], []\n",
        "    for task_id, task in tasks.items():\n",
        "        train_inputs, train_outputs, _ = extract_task_data(task)\n",
        "        if train_inputs is None or train_outputs is None:\n",
        "            continue\n",
        "        for inp, out in zip(train_inputs, train_outputs):\n",
        "            inp_padded = pad_grid(inp)\n",
        "            out_padded = pad_grid(out)\n",
        "            x_train.append(inp_padded)\n",
        "            y_train.append(out_padded)\n",
        "\n",
        "    x_train = np.array(x_train).reshape(-1, 30, 30, 1)  # Add channel dimension\n",
        "    y_train = np.array(y_train).reshape(-1, 30, 30)      # Use integer class labels for sparse_categorical_crossentropy\n",
        "\n",
        "    return x_train, y_train\n",
        "\n",
        "# Define the ARC model architecture\n",
        "def create_arc_model(input_shape):\n",
        "    model = models.Sequential([\n",
        "        layers.Input(shape=input_shape),\n",
        "        layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "        layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "        layers.Conv2D(256, (3, 3), activation='relu', padding='same'),\n",
        "        layers.Flatten(),\n",
        "        layers.Dense(30 * 30 * 10, activation='softmax'),  # Output 10 classes per pixel\n",
        "        layers.Reshape((30, 30, 10))  # Reshape to (30, 30, 10)\n",
        "    ])\n",
        "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# File paths\n",
        "train_path = \"/kaggle/input/arc-prize-2024/arc-agi_training_challenges.json\"\n",
        "eval_path = \"/kaggle/input/arc-prize-2024/arc-agi_evaluation_challenges.json\"\n",
        "\n",
        "# Load datasets\n",
        "train_data = load_arc_data(train_path)\n",
        "eval_data = load_arc_data(eval_path)\n",
        "\n",
        "# Prepare training data\n",
        "x_train, y_train = prepare_data(train_data)\n",
        "\n",
        "# Create and summarize the model\n",
        "input_shape = (30, 30, 1)  # Input shape (30, 30, 1)\n",
        "model = create_arc_model(input_shape)\n",
        "model.summary()\n",
        "\n",
        "# Train the model\n",
        "model.fit(x_train, y_train, epochs=10, batch_size=32, validation_split=0.2)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-10T20:29:20.79853Z",
          "iopub.execute_input": "2024-09-10T20:29:20.799862Z",
          "iopub.status.idle": "2024-09-10T20:39:39.871093Z",
          "shell.execute_reply.started": "2024-09-10T20:29:20.799807Z",
          "shell.execute_reply": "2024-09-10T20:39:39.869853Z"
        },
        "trusted": true,
        "id": "-bZ4ozmeBB_-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict the output for a test grid\n",
        "def predict_output(model, test_input):\n",
        "    test_input_padded = pad_grid(test_input).reshape(1, 30, 30, 1)\n",
        "    predicted_output = model.predict(test_input_padded)\n",
        "    predicted_output = np.argmax(predicted_output, axis=-1).reshape(30, 30)\n",
        "    return predicted_output\n",
        "\n",
        "# Create submission JSON file\n",
        "def create_submission(tasks, model):\n",
        "    submission = []\n",
        "    for task_id, task in tasks.items():\n",
        "        test_inputs = extract_task_data(task)[2]\n",
        "        if test_inputs is None:\n",
        "            continue\n",
        "        task_predictions = []\n",
        "        for test_input in test_inputs:\n",
        "            predicted_output_1 = predict_output(model, test_input)\n",
        "            predicted_output_2 = predict_output(model, test_input)  # Another strategy for second prediction\n",
        "            task_predictions.append({\n",
        "                \"attempt_1\": predicted_output_1.tolist(),\n",
        "                \"attempt_2\": predicted_output_2.tolist()\n",
        "            })\n",
        "        submission.append({\n",
        "            \"task_id\": task_id,\n",
        "            \"predictions\": task_predictions\n",
        "        })\n",
        "\n",
        "    print(submission)\n",
        "\n",
        "    with open('submission.json', 'w') as f:\n",
        "        json.dump(submission, f)\n",
        "\n",
        "# Create submission file\n",
        "create_submission(eval_data, model)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-10T20:46:08.992142Z",
          "iopub.execute_input": "2024-09-10T20:46:08.993231Z",
          "iopub.status.idle": "2024-09-10T20:47:34.698986Z",
          "shell.execute_reply.started": "2024-09-10T20:46:08.993151Z",
          "shell.execute_reply": "2024-09-10T20:47:34.697686Z"
        },
        "trusted": true,
        "id": "dsaBxo-sBB_-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XxbGBw3IBB_-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}